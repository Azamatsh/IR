{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from elasticsearch import Elasticsearch\n",
    "from elasticsearch.helpers import bulk, parallel_bulk\n",
    "import ir_measures\n",
    "from ir_measures import *\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "es = Elasticsearch(hosts='https://localhost:9200', \n",
    "                     basic_auth=('elastic', 'sYV-CgqebNRTw1e=L=pY'),\n",
    "                     verify_certs=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WikiIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('wikIR1k/documents.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'acknowledged': True, 'shards_acknowledged': True, 'index': 'wiki'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Without stemming\n",
    "\n",
    "mappings = {\n",
    "    'properties': {\n",
    "        'text': {\n",
    "            'type': 'text',\n",
    "            'analyzer': 'white'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    'analysis' : {\n",
    "        'analyzer' : {\n",
    "            'white' : {\n",
    "                'tokenizer' : 'whitespace'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "index = 'wiki'\n",
    "\n",
    "if es.indices.exists(index=index):\n",
    "    es.indices.delete(index=index)\n",
    "es.indices.create(index=index, settings=settings, mappings=mappings)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Tests', 'for', 'small', 'assignment']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def check_analyzer(analyzer, text):\n",
    "    body = analyzer\n",
    "    body['text'] = text\n",
    "    tokens = es.indices.analyze(index=index, body=body)['tokens']\n",
    "    tokens = [token_info['token'] for token_info in tokens]\n",
    "    return tokens\n",
    "\n",
    "text = 'Tests for small assignment'\n",
    "analyzer = {\n",
    "    'analyzer': 'white'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369721/369721 [00:24<00:00, 15328.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing time: 24.41376805305481\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Indexing documents\n",
    "\n",
    "def create_es_action(index, doc_id, document):\n",
    "    return {\n",
    "        '_index': index,\n",
    "        '_id': doc_id,\n",
    "        '_source': document\n",
    "    }\n",
    "\n",
    "\n",
    "def es_action_generator(df):\n",
    "    for doc_id, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "        doc = {'text': row['text_right']}\n",
    "        yield create_es_action(index, row['id_right'], doc)\n",
    "\n",
    "\n",
    "start = time()\n",
    "for ok, result in parallel_bulk(es, es_action_generator(df), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "stop = time()\n",
    "print('Indexing time:', stop - start)\n",
    "        \n",
    "es.indices.refresh(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Test', 'for', 'small', 'assign']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with stemming\n",
    "\n",
    "mappings = {\n",
    "    'properties': {\n",
    "        'text': {\n",
    "            'type': 'text',\n",
    "            'analyzer': 'porter_stemmer'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "settings = {\n",
    "    'analysis' : {\n",
    "        'analyzer' : {\n",
    "            'porter_stemmer' : {\n",
    "                'tokenizer' : 'whitespace',\n",
    "                'filter' : ['porter_stem']\n",
    "            }\n",
    "    },\n",
    "        'filter' : {\n",
    "            'porter_stem' : {\n",
    "                'type' : 'porter_stem',\n",
    "                'language' : 'English'\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "index = 'wiki'\n",
    "\n",
    "if es.indices.exists(index=index):\n",
    "    es.indices.delete(index=index)\n",
    "es.indices.create(index=index, settings=settings, mappings=mappings)\n",
    "\n",
    "\n",
    "analyzer = {\n",
    "    'analyzer': 'porter_stemmer'\n",
    "}\n",
    "\n",
    "check_analyzer(analyzer, text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369721/369721 [00:25<00:00, 14425.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing time: 25.94707989692688\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start = time()\n",
    "for ok, result in parallel_bulk(es, es_action_generator(df), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "stop = time()\n",
    "\n",
    "print('Indexing time:', stop-start)\n",
    "        \n",
    "es.indices.refresh(index=index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_result(search_result, fields=[]):\n",
    "    res = search_result['hits']\n",
    "    print(f'Total documents: {res[\"total\"][\"value\"]}')\n",
    "    for hit in res['hits']:\n",
    "        print(f'Doc {hit[\"_id\"]}, score is {hit[\"_score\"]}')\n",
    "        for field in fields:\n",
    "            print(f'{field}: {hit[\"_source\"][field]}')\n",
    "    \n",
    "def search(query, *args):\n",
    "    return pretty_print_result(es.search(index=index, body=query, size=20), args)\n",
    "\n",
    "def get_doc_by_id(doc_id):\n",
    "    return es.get(index=index, id=doc_id)['_source']\n",
    "\n",
    "def search_results(query_id, query):\n",
    "    res = es.search(index=index, body=query, size=20)['hits']\n",
    "    return [(str(query_id), str(hit['_id']), hit['_score'], rank) for rank, hit in enumerate(res['hits'])]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_queries = pd.read_csv('queries.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def make_query(text):\n",
    "    return {\n",
    "        'query': {\n",
    "            'bool': {\n",
    "                'must': {\n",
    "                    'match': {\n",
    "                        'text': text\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "def score(test):\n",
    "    result = []\n",
    "    file = open('data.res', 'w')\n",
    "    for i, row in test.iterrows():\n",
    "        for res in search_results(row['id_left'], make_query(row['text_left'])):\n",
    "            result.append(ir_measures.ScoredDoc(res[0], res[1], res[2]))\n",
    "            file.write(f'{res[0]} Q0 {res[1]} {res[3]} {res[2]} BM25\\n')\n",
    "    file.close()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query execution time (total): 0.9339628219604492 s\n"
     ]
    }
   ],
   "source": [
    "run = score(test_queries)\n",
    "start = time()\n",
    "score(test_queries)\n",
    "stop = time()\n",
    "\n",
    "print('Query execution time (total):', stop-start, 's')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spacy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# conda install -c conda-forge spacy\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# import spacy\u001b[39;00m\n\u001b[1;32m      4\u001b[0m docs_list \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m----> 5\u001b[0m nlp \u001b[38;5;241m=\u001b[39m \u001b[43mspacy\u001b[49m\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124men_core_web_sm\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df\u001b[38;5;241m.\u001b[39miterrows(), total\u001b[38;5;241m=\u001b[39mdf\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m      8\u001b[0m     nlp_doc \u001b[38;5;241m=\u001b[39m nlp(row[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtext_right\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'spacy' is not defined"
     ]
    }
   ],
   "source": [
    "# conda install -c conda-forge spacy\n",
    "# import spacy\n",
    "\n",
    "docs_list = []\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "for i, row in tqdm(df.iterrows(), total=df.shape[0]):\n",
    "    nlp_doc = nlp(row['text_right'])\n",
    "    new_doc = ''\n",
    "    for token in nlp_doc:\n",
    "        new_doc = new_doc + ' ' + token.lemma_\n",
    "    docs_list.append(new_doc)\n",
    "\n",
    "lemmatized = pd.DataFrame({'id_right': df['id_right'].values, 'text_right': docs_list})\n",
    "lemmatized.to_csv('optional_part.csv', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized = pd.read_csv('optional_part.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Queries (lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 369721/369721 [00:24<00:00, 14896.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing time: 25.227459192276\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ObjectApiResponse({'_shards': {'total': 2, 'successful': 1, 'failed': 0}})"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if es.indices.exists(index=index):\n",
    "    es.indices.delete(index=index)\n",
    "es.indices.create(index=index, settings=settings, mappings=mappings)\n",
    "\n",
    "\n",
    "start = time()\n",
    "for ok, result in parallel_bulk(es, es_action_generator(lemmatized), queue_size=4, thread_count=4, chunk_size=1000):\n",
    "    if not ok:\n",
    "        print(result)\n",
    "stop = time()\n",
    "\n",
    "print('Indexing time:', stop-start)\n",
    "        \n",
    "es.indices.refresh(index=index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@20: 0.09499999999999999,\n",
       " AP: 0.11196168401599797,\n",
       " P@5: 0.18399999999999994,\n",
       " P@10: 0.1319999999999999}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BM25 results\n",
    "BM25 = ir_measures.read_trec_run('BM25.res')\n",
    "qrels = ir_measures.read_trec_qrels('qrels')\n",
    "\n",
    "ir_measures.calc_aggregate([P@5, P@10, P@20, AP], qrels, BM25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@20: 0.14350000000000002,\n",
       " AP: 0.14653833730973906,\n",
       " P@5: 0.3119999999999997,\n",
       " P@10: 0.20999999999999994}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = ir_measures.read_trec_run('data.res')\n",
    "qrels = ir_measures.read_trec_qrels('qrels')\n",
    "\n",
    "ir_measures.calc_aggregate([P@5, P@10, P@20, AP], qrels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{P@20: 0.1315,\n",
       " AP: 0.1212078886744036,\n",
       " P@5: 0.24399999999999986,\n",
       " P@10: 0.18499999999999994}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def make_query(text):\n",
    "    return {\n",
    "        'query':{\n",
    "            \"bool\": {\n",
    "                'must': {\n",
    "                    'match': {\n",
    "                        'text': text\n",
    "                    }             \n",
    "                },\n",
    "                'should': {\n",
    "                    \"match_phrase\": {\n",
    "                        \"text\": {\n",
    "                            \"query\": text,\n",
    "                            \"boost\": 5\n",
    "                        }\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "\n",
    "results = ir_measures.read_trec_run('lem_test_query.res')\n",
    "qrels = ir_measures.read_trec_qrels('qrels')\n",
    "\n",
    "ir_measures.calc_aggregate([P@5, P@10, P@20, AP], qrels, results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
